from bs4 import BeautifulSoup
import os
import openai
from typing import Optional, Dict, Any
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# --- ì„¤ì •ê°’ ---
BASE_URL = "https://www.hankyung.com"
NEWS_WALL_URL = f"{BASE_URL}/globalmarket/news-wallstreet-now"
ARTICLE_BODY_SELECTOR = '#articletxt' 

# === ìµœì¢… í™•ì •ëœ í•µì‹¬ ì„ íƒì ê·¸ë£¹ (ì‚¬ìš©ì ì œê³µ HTML ê¸°ë°˜) ===
ARTICLE_ITEM_SELECTOR = 'ul.news-list li' 
TITLE_SELECTOR = '.news-tit a'
DATE_SELECTOR = '.txt-date' 
# =================================================

"""
python page/stock_summary.py 2025.10.23 ì§€ì •ëœ ë‚ ì§œë¡œ í…ŒìŠ¤íŠ¸ ìš”ì•½ ì‹¤í–‰í•˜ê¸°
"""
def summarize_text(text: str) -> str:
    """ChatGPTAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""
    
    if not text:
        return "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    # ë¡œì»¬ ë° ì‚¬ì´íŠ¸ ì¡°ê±´ ì¶”ê°€
    api_key = os.getenv('OPENAI_API_KEY') or os.getenv('CHATGPT_API_KEY')
    if not api_key:
        return "âŒ ì˜¤ë¥˜: OPENAI_API_KEY ë˜ëŠ” CHATGPT_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ ChatGPT APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

    prompt = (
        "ë‹¤ìŒ [ê¸°ì‚¬ ë³¸ë¬¸]ì„ í•œêµ­ì–´ë¡œ ì½ê³ , í•µì‹¬ ë‚´ìš©ì„ 3~4ê°œì˜ ì£¼ìš” ì£¼ì œë¡œ ë‚˜ëˆ„ì–´ ìš”ì•½í•´. "
        "ê° ì£¼ì œëŠ” ë°˜ë“œì‹œ '1. 2. 3.'ê³¼ ê°™ì´ ë²ˆí˜¸ë¡œ ì‹œì‘í•´ì•¼ í•˜ë©°, [ ] ê´„í˜¸ ì—†ì´ ì‘ì„±í•´. "
        "ìš”ì•½ì€ ì•„ë˜ ì˜ˆì‹œì™€ ê°™ì´ 'ë²ˆí˜¸. ì£¼ì œ'ì™€ ê·¸ì— ëŒ€í•œ 2~3ì¤„ì˜ í•µì‹¬ ë‚´ìš© êµ¬ì¡°ë¥¼ ë”°ë¼. "
        "ë¶ˆí•„ìš”í•œ ì„œë¡ ì´ë‚˜ ê²°ë¡  ì—†ì´, ë°”ë¡œ ì£¼ì œì™€ ìš”ì•½ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ëª¨ë“  ë‚´ìš©ì€ ê°„ê²°í•˜ê³ , ì¡´ëŒ€ë§(ê³µì†¡í•œ ë§íˆ¬)ì‘ì„±."
        "ì „ì²´ ìš”ì•½ ê²°ê³¼ê°€ 600 í† í° ì´ë‚´ë¡œ ë‚˜ì˜¤ë„ë¡ ë¶„ëŸ‰ì„ ì¡°ì ˆ.\n\n"
        "[ì˜ˆì‹œ í˜•ì‹]\n"
        "1. 9ì›” CPI ë°œí‘œì™€ ê¸ˆë¦¬ ì¸í•˜ ê¸°ëŒ€\n"
        "9ì›” CPI ë°ì´í„°ê°€ ì˜ˆìƒë³´ë‹¤ ì¢‹ì•˜ìœ¼ë©°, ê´€ì„¸ ì¸í”Œë ˆì´ì…˜ ìš°ë ¤ê°€ ì—†ìŒì„ ë³´ì—¬ì¤¬ë‹¤ê³  ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\n"
        f"{text}"
    )
    
    try:
        client = openai.OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ìš”ì•½ ê²½ì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=600, # ìš”ì•½ ê²°ê³¼ê°€ 600 í† í° ì´ë‚´ë¡œ ì œí•œ
            temperature=0.5,
        )
        content = response.choices[0].message.content if response.choices and response.choices[0].message else None
        if content:
            return content.strip()
        else:
            return "âŒ ChatGPT API ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"âŒ ChatGPT API í˜¸ì¶œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
import requests
from bs4 import BeautifulSoup
from datetime import datetime

"""
playwright ì—†ì´ íŒŒì‹±í•˜ì—¬ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œ
"""

# ...ê¸°ì¡´ ìƒìˆ˜ ë° summarize_text í•¨ìˆ˜ ìœ ì§€...

def get_latest_article_requests(target_date: str = None) -> dict | None:
    if target_date is None:
        target_date = datetime.now().strftime("%Y.%m.%d")
    print(f"[ì •ë³´] {target_date} ë‚ ì§œ ê¸°ì‚¬ë¥¼ ìš°ì„  ì°¾ê³  ìˆìŠµë‹ˆë‹¤.")
    try:
        resp = requests.get(NEWS_WALL_URL, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        news_list = soup.select(ARTICLE_ITEM_SELECTOR)
        latest_article = None
        for i, item in enumerate(news_list):
            title_tag = item.select_one(TITLE_SELECTOR)
            date_tag = item.select_one(DATE_SELECTOR)
            if title_tag and date_tag:
                article_date = date_tag.get_text(strip=True).split()[0]
                article_url = title_tag.get('href')
                if not article_url or not isinstance(article_url, str):
                    continue
                if not article_url.startswith('http'):
                    article_url = BASE_URL + article_url
                current_article = {
                    'title': title_tag.get_text(strip=True),
                    'url': article_url,
                    'date': article_date
                }
                if i == 0:
                    latest_article = current_article
                if article_date == target_date:
                    return current_article
        # ì›í•˜ëŠ” ë‚ ì§œ ê¸°ì‚¬ ì—†ìœ¼ë©´ ìµœì‹  ê¸°ì‚¬ ë°˜í™˜
        if latest_article:
            print(f"{target_date} ê¸°ì‚¬ ì—†ìŒ, ìµœì‹  ê¸°ì‚¬ë¡œ ëŒ€ì²´")
            return latest_article
        print(f"{target_date} ê¸°ì‚¬ ì—†ìŒ")
        return None
    except Exception as e:
        print(f"âŒ ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def get_article_content_requests(url: str) -> str | None:
    try:
        resp = requests.get(url, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        content_element = soup.select_one(ARTICLE_BODY_SELECTOR)
        if content_element:
            for ad_tag in content_element.find_all(class_='atc-ad-area'):
                ad_tag.decompose()
            for script_tag in content_element.find_all('script'):
                script_tag.decompose()
            return content_element.get_text('\n', strip=True)
        return None
    except Exception as e:
        print(f"âŒ ê¸°ì‚¬ ë³¸ë¬¸ ì ‘ì† ë˜ëŠ” ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {e}")
        return None

def main():
    import sys
    print("ğŸ“° í•œê²½ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ë‚˜ìš° ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸ (requests) ì‹¤í–‰ ì‹œì‘")
    # ì¸ìë¡œ ë‚ ì§œ(YYYY.MM.DD) ë°›ê¸°, ì—†ìœ¼ë©´ ì˜¤ëŠ˜
    target_date = None
    if len(sys.argv) > 1:
        target_date = sys.argv[1]
    article = get_latest_article_requests(target_date)
    if not article:
        print("START_SUMMARY_BODY")
        print(f"{target_date or 'ì˜¤ëŠ˜'} ê¸°ì‚¬ ì—†ìŒ")
        print("END_SUMMARY_BODY")
        return
    print(f"\nâœ… ê¸°ì‚¬ 1ê°œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. (ë‚ ì§œ: {article['date']})")
    print(f"  â”” ì œëª©: {article['title']}")
    print(f"  â”” URL: {article['url']}")
    content = get_article_content_requests(article['url'])
    if content:
        print("[ì •ë³´] ChatGPTAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ ìš”ì²­ ì¤‘...")
        summary = summarize_text(content)
        print("START_SUMMARY_BODY", flush=True)
        print(summary, flush=True)
        print("END_SUMMARY_BODY", flush=True)
        print(f"URL: {article['url']}", flush=True)
    else:
        print("START_SUMMARY_BODY")
        print("ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("END_SUMMARY_BODY")

if __name__ == "__main__":
    main()


"""
playwright vercel ë°°í¬ ì‹œ ìš©ëŸ‰ 250mb ì œí•œìœ¼ë¡œ ì¸í•´ ì‚­ì œ
"""
# async def get_latest_article_playwright(page) -> Optional[Dict[str, Any]]:
#     """
#     Playwright í˜ì´ì§€ ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  ë¡œë”©ëœ í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
#     (ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ë¥¼ ìš°ì„  ì°¾ê³ , ì—†ìœ¼ë©´ ëª©ë¡ì˜ ë§¨ ìœ„ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.)
#     """
#     today_date = datetime.now().strftime("%Y.%m.%d")
#     print(f"[ì •ë³´] ì˜¤ëŠ˜ ({today_date}) ë‚ ì§œ ê¸°ì‚¬ë¥¼ ìš°ì„  ì°¾ê³  ìˆìŠµë‹ˆë‹¤.")

#     try:
#         await page.goto(NEWS_WALL_URL, wait_until='domcontentloaded') 
#         await page.locator(ARTICLE_ITEM_SELECTOR).first.wait_for(state="attached", timeout=15000)

#         content = await page.content()
#         soup = BeautifulSoup(content, 'html.parser')

#         news_list = soup.select(ARTICLE_ITEM_SELECTOR) 
        
#         if not news_list:
#             print("âŒ ê¸°ì‚¬ ëª©ë¡ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœì¢… ì„ íƒì í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
#             return None
        
#         # --- ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ë° ë¡œì§ ìˆ˜ì • ---
#         target_article = None
        
#         for i, item in enumerate(news_list):
#             title_tag = item.select_one(TITLE_SELECTOR)
#             date_tag = item.select_one(DATE_SELECTOR)
            
#             if title_tag and date_tag:
#                 title = title_tag.get_text(strip=True)
#                 relative_url = title_tag.get('href')
                
#                 article_url = relative_url
#                 if not article_url.startswith('http'):
#                     article_url = BASE_URL + article_url

#                 full_date_time = date_tag.get_text(strip=True)
#                 article_date = full_date_time.split()[0]
                
#                 current_article = {
#                     'title': title,
#                     'url': article_url,
#                     'date': article_date
#                 }

#                 # 1. ëª©ë¡ì˜ ë§¨ ìœ„ì— ìˆëŠ” ê¸°ì‚¬ë¥¼ ë¬´ì¡°ê±´ ì €ì¥ (ê°€ì¥ ìµœì‹  ê¸°ì‚¬)
#                 if i == 0:
#                     target_article = current_article
                
#                 # 2. ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ë¥¼ ë°œê²¬í•˜ë©´ ê·¸ê²ƒì„ ìµœì¢… íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •í•˜ê³  ë°˜ë³µ ì¢…ë£Œ
#                 if article_date == today_date:
#                     target_article = current_article
#                     break # ì˜¤ëŠ˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìœ¼ë‹ˆ ë°˜ë³µì„ ë©ˆì¶¥ë‹ˆë‹¤.
        
#         if target_article:
#             if target_article['date'] == today_date:
#                  print(f"âœ… ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬({today_date})ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
#             else:
#                  print(f"âš ï¸ ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ê°€ ì—†ì–´, ê°€ì¥ ìµœì‹  ë‚ ì§œ({target_article['date']}) ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
#             return target_article
        
#         return None

#     except Exception as e:
#         print(f"âŒ ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return None


# async def get_article_content_playwright(page, url: str) -> Optional[str]:
#     """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
#     try:
#         await page.goto(url, wait_until='domcontentloaded')
        
#         await page.locator(ARTICLE_BODY_SELECTOR).wait_for(state="attached", timeout=15000)

#         content = await page.content()
#         soup = BeautifulSoup(content, 'html.parser')
#         content_element = soup.select_one(ARTICLE_BODY_SELECTOR)
        
#         if content_element:
#             for ad_tag in content_element.find_all(class_='atc-ad-area'):
#                 ad_tag.decompose()
#             for script_tag in content_element.find_all('script'):
#                 script_tag.decompose()
                
#             content = content_element.get_text('\n', strip=True)
#             return content
#         else:
#             return None

#     except Exception as e:
#         print(f"âŒ ê¸°ì‚¬ ë³¸ë¬¸ ì ‘ì† ë˜ëŠ” ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {e}")
#         return None

# async def main_async():
#     """ë©”ì¸ ë¹„ë™ê¸° í•¨ìˆ˜: Playwrightë¡œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™€ ìš”ì•½í•©ë‹ˆë‹¤."""
#     print("ğŸ“° í•œê²½ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ë‚˜ìš° ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸ (Playwright) ì‹¤í–‰ ì‹œì‘")
    
#     async with async_playwright() as p:
#         browser = await p.chromium.launch(headless=True)
#         page = await browser.new_page()

#         try:
#             # 1. ì˜¤ëŠ˜ ë˜ëŠ” ê°€ì¥ ìµœì‹  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
#             article = await get_latest_article_playwright(page)

#             if not article:
#                 print("\n[ì™„ë£Œ] í¬ë¡¤ë§í•  ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
#                 return

#             print(f"\nâœ… ê¸°ì‚¬ 1ê°œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. (ë‚ ì§œ: {article['date']})")
#             print(f"  â”” ì œëª©: {article['title']}")
#             print(f"  â”” URL: {article['url']}")
            
#             # 2. ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
#             print("\n[ì •ë³´] ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œ ì¤‘...")
#             content = await get_article_content_playwright(page, article['url'])
            
#             if content:
#                 # 3. ChatGPTAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìˆ˜í–‰
#                 print("[ì •ë³´] ChatGPTAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ ìš”ì²­ ì¤‘...")
#                 summary = summarize_text(content)
                
#                 # 4. ê²°ê³¼ ì¶œë ¥: êµ¬ë¶„ì„ ì„ ì œê±°í•˜ê³  ê¹”ë”í•œ ì‹œì‘/ë íƒœê·¸ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
#                 print("START_SUMMARY_BODY", flush=True)
#                 print(summary, flush=True)
#                 print("END_SUMMARY_BODY", flush=True)
#                 print(f"URL: {article['url']}", flush=True)
#             else:
#                 print("\nâŒ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í•˜ì—¬ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
#         finally:
#             await browser.close()